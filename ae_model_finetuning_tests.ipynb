{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model updated for TF2.0\n",
    "#python -m ae_model_train --batchsize 100 --cvfold 0 --alpha_T 1.0 --alpha_E 1.0 --alpha_M 1.0 --lambda_TE 0.0 --latent_dim 3 --n_epochs 2000 --n_steps_per_epoch 500 --ckpt_save_freq 100 --run_iter 0 --model_id 'v1' --exp_name 'TE_Patchseq_Bioarxiv'\n",
    "import argparse\n",
    "import os\n",
    "import pdb\n",
    "import re\n",
    "import socket\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from data_funcs import TEM_get_splits\n",
    "from ae_model_def import Model_TE\n",
    "import csv\n",
    "from timebudget import timebudget\n",
    "\n",
    "from ae_model_train import Datagen\n",
    "from ae_model_train import set_paths\n",
    "\n",
    "batchsize=200\n",
    "cvfold=0\n",
    "alpha_T=1.0\n",
    "alpha_E=1.0\n",
    "alpha_M=1.0\n",
    "lambda_TE=1.0\n",
    "latent_dim=3\n",
    "n_epochs=100\n",
    "n_steps_per_epoch=500\n",
    "ckpt_save_freq=100\n",
    "run_iter=0\n",
    "model_id='Frozen_net'\n",
    "exp_name='TE_Patchseq_Bioarxiv'\n",
    "    \n",
    "dir_pth = set_paths(exp_name=exp_name)\n",
    "fileid = model_id + \\\n",
    "    '_aT_' + str(alpha_T) + \\\n",
    "    '_aE_' + str(alpha_E) + \\\n",
    "    '_aM_' + str(alpha_M) + \\\n",
    "    '_cs_' + str(lambda_TE) + \\\n",
    "    '_ld_' + str(latent_dim) + \\\n",
    "    '_bs_' + str(batchsize) + \\\n",
    "    '_se_' + str(n_steps_per_epoch) +\\\n",
    "    '_ne_' + str(n_epochs) + \\\n",
    "    '_cv_' + str(cvfold) + \\\n",
    "    '_ri_' + str(run_iter)\n",
    "fileid = fileid.replace('.', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data:\n",
    "D = sio.loadmat(dir_pth['data']+'PS_v4_beta_0-4_matched_well-sampled.mat',squeeze_me=True)\n",
    "cvset,testset = TEM_get_splits(D)\n",
    "\n",
    "train_ind = cvset[cvfold]['train']\n",
    "train_T_dat = tf.constant(D['T_dat'][train_ind,:])\n",
    "train_E_dat = D['E_dat'][train_ind,:]\n",
    "train_M_dat = D['M_dat'][train_ind]\n",
    "train_E_dat = tf.constant(np.concatenate([train_E_dat,train_M_dat.reshape(train_M_dat.size,1)],axis=1))\n",
    "\n",
    "val_ind = cvset[cvfold]['val']\n",
    "val_T_dat = D['T_dat'][val_ind,:]\n",
    "val_E_dat = D['E_dat'][val_ind,:]\n",
    "val_M_dat = D['M_dat'][val_ind]\n",
    "val_E_dat = tf.constant(np.concatenate([val_E_dat,val_M_dat.reshape(val_M_dat.size,1)],axis=1))\n",
    "Xval = (tf.constant(val_T_dat),tf.constant(val_E_dat))\n",
    "\n",
    "maxsteps = tf.constant(n_epochs*n_steps_per_epoch)\n",
    "batchsize = tf.constant(batchsize)\n",
    "alpha_T   = tf.constant(alpha_T,dtype=tf.float32)\n",
    "alpha_E   = tf.constant(alpha_E,dtype=tf.float32)\n",
    "alpha_M   = tf.constant(alpha_M,dtype=tf.float32)\n",
    "lambda_TE = tf.constant(lambda_TE,dtype=tf.float32)\n",
    "\n",
    "def min_var_loss(zi, zj, Wij=None):\n",
    "    \"\"\"SVD is calculated over entire batch. MSE is calculated over only paired entries within batch\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(zi)[0]\n",
    "    if Wij is None:\n",
    "        Wij_ = tf.ones([batch_size, ])\n",
    "    else:\n",
    "        Wij_ = tf.reshape(Wij, [batch_size, ])\n",
    "\n",
    "    zi_paired = tf.boolean_mask(zi, tf.math.greater(Wij_, 1e-2))\n",
    "    zj_paired = tf.boolean_mask(zj, tf.math.greater(Wij_, 1e-2))\n",
    "    Wij_paired = tf.boolean_mask(Wij_, tf.math.greater(Wij_, 1e-2))\n",
    "\n",
    "    vars_j_ = tf.square(tf.linalg.svd(zj - tf.reduce_mean(zj, axis=0), compute_uv=False))/tf.cast(batch_size - 1, tf.float32)\n",
    "    vars_j  = tf.where(tf.math.is_nan(vars_j_), tf.zeros_like(vars_j_) + tf.cast(1e-2,dtype=tf.float32), vars_j_)\n",
    "    weighted_distance = tf.multiply(tf.sqrt(tf.reduce_sum(tf.math.squared_difference(zi_paired, zj_paired),axis=1)),Wij_paired)\n",
    "    loss_ij = tf.reduce_mean(weighted_distance,axis=None)/tf.maximum(tf.reduce_min(vars_j, axis=None),tf.cast(1e-2,dtype=tf.float32))\n",
    "    return loss_ij\n",
    "\n",
    "def report_losses(XT, XE, zT, zE, XrT, XrE,epoch, datatype='train', verbose=False):\n",
    "    mse_loss_T = tf.reduce_mean(tf.math.squared_difference(XT, XrT))\n",
    "    mse_loss_E = tf.reduce_mean(tf.math.squared_difference(XE, XrE))\n",
    "    mse_loss_M = tf.reduce_mean(tf.math.squared_difference(XE[:, -1], XrE[:, -1]))\n",
    "    mse_loss_TE = tf.reduce_mean(tf.math.squared_difference(zT, zE))\n",
    "\n",
    "    if verbose:\n",
    "        print('Epoch:{:5d}, '\n",
    "              'mse_T: {:0.5f}, '\n",
    "              'mse_E: {:0.5f}, '\n",
    "              'mse_M: {:0.5f}, '\n",
    "              'mse_TE: {:0.5f}'.format(epoch,\n",
    "                                       mse_loss_T.numpy(),\n",
    "                                       mse_loss_E.numpy(),\n",
    "                                       mse_loss_M.numpy(),\n",
    "                                       mse_loss_TE.numpy()))\n",
    "\n",
    "    log_name = [datatype+i for i in ['epoch','mse_T', 'mse_E', 'mse_M', 'mse_TE']]\n",
    "    log_values = [epoch, mse_loss_T.numpy(), mse_loss_E.numpy(),\n",
    "                  mse_loss_M.numpy(), mse_loss_TE.numpy()]\n",
    "    return log_name, log_values\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "train_generator = tf.data.Dataset.from_generator(Datagen,output_types=(tf.float32, tf.float32),\n",
    "                                                 args=(maxsteps,batchsize,train_T_dat,train_E_dat))\n",
    "\n",
    "model_TE = Model_TE(T_output_dim=train_T_dat.shape[1],\n",
    "                    E_output_dim=train_E_dat.shape[1],\n",
    "                    T_intermediate_dim=50,\n",
    "                    E_intermediate_dim=40,\n",
    "                    T_dropout=0.5,\n",
    "                    E_gnoise_sd=0.05,\n",
    "                    E_dropout=0.1,\n",
    "                    latent_dim=latent_dim,\n",
    "                    name='TE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform dummy inference to build the model:\n",
    "x = tf.constant(np.random.rand(1,train_T_dat.shape[1]),dtype=tf.float32)\n",
    "y = tf.constant(np.random.rand(1,train_E_dat.shape[1]),dtype=tf.float32)\n",
    "_,_,_,_ = model_TE((x,y),train_T=False,train_E=False)\n",
    "\n",
    "#Loading weights:\n",
    "model_TE.load_weights('/Users/fruity/Dropbox/AllenInstitute/CellTypes/dat/result/TE_Patchseq_Bioarxiv/v1_aT_1-0_aE_1-0_aM_1-0_cs_1-0_ld_3_bs_200_se_500_ne_1500_cv_0_ri_0-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_fn(XT, XE, train_T=False, train_E=False, subnetwork='all'):\n",
    "    with tf.GradientTape() as tape:\n",
    "        zT, zE, XrT, XrE = model_TE((XT, XE), train_T=train_T, train_E=train_E)\n",
    "        \n",
    "        #Find the weights to update\n",
    "        mse_loss_T = tf.reduce_mean(tf.math.squared_difference(XT, XrT))\n",
    "        mse_loss_E = tf.reduce_mean(tf.math.squared_difference(XE[:, :-1], XrE[:, :-1]))\n",
    "        mse_loss_M = tf.reduce_mean(tf.math.squared_difference(XE[:, -1], XrE[:, -1]))\n",
    "        cpl_loss_TE = min_var_loss(zT, zE)\n",
    "        loss = alpha_T*mse_loss_T + \\\n",
    "            alpha_E*mse_loss_E + \\\n",
    "            alpha_M*mse_loss_M + \\\n",
    "            lambda_TE*cpl_loss_TE\n",
    "\n",
    "        #Apply updates if training any of the subnetworks\n",
    "        if subnetwork is 'all':\n",
    "            trainable_weights = [weight for weight in model_TE.trainable_weights]\n",
    "            grads = tape.gradient(loss, trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "            \n",
    "        if subnetwork is 'E':\n",
    "            trainable_weights = [weight for weight in model_TE.trainable_weights if '_E' in weight.name]\n",
    "            grads = tape.gradient(loss, trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "            \n",
    "    return zT, zE, XrT, XrE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def finetune_fn(XT, XE, train_T=False, train_E=False, subnetwork=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        zT, zE, XrT, XrE = model_TE((XT, XE), train_T=train_T, train_E=train_E)\n",
    "\n",
    "        #Find the weights to update\n",
    "        if subnetwork=='Encoder_E':\n",
    "            mse_loss_z = tf.reduce_mean(tf.math.squared_difference(zT,zE))        \n",
    "            loss = mse_loss_z\n",
    "            trainable_weights = [weight for weight in model_TE.trainable_weights if subnetwork in weight.name]\n",
    "            grads = tape.gradient(loss, trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "\n",
    "        elif subnetwork=='Decoder_E':\n",
    "            mse_loss_E = tf.reduce_mean(tf.math.squared_difference(XE[:, :-1], XrE[:, :-1]))\n",
    "            mse_loss_M = tf.reduce_mean(tf.math.squared_difference(XE[:, -1], XrE[:, -1]))\n",
    "            loss = mse_loss_M+mse_loss_E\n",
    "            trainable_weights = [weight for weight in model_TE.trainable_weights if subnetwork in weight.name]\n",
    "            grads = tape.gradient(loss, trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "\n",
    "    return zT, zE, XrT, XrE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect everything to change:--------------\n",
      "Epoch:    0, mse_T: 1.74592, mse_E: 0.40934, mse_M: 0.03344, mse_TE: 0.01526\n",
      "Epoch:    1, mse_T: 1.74258, mse_E: 0.40608, mse_M: 0.03984, mse_TE: 0.01496\n",
      "Epoch:    2, mse_T: 1.74459, mse_E: 0.40853, mse_M: 0.03456, mse_TE: 0.01774\n",
      "Epoch:    3, mse_T: 1.74090, mse_E: 0.40217, mse_M: 0.03627, mse_TE: 0.01507\n",
      "Epoch:    4, mse_T: 1.74253, mse_E: 0.40386, mse_M: 0.03349, mse_TE: 0.01506\n",
      "\n",
      "Expect mse_T to be unchanged:--------------\n",
      "Epoch:    0, mse_T: 1.72686, mse_E: 0.40256, mse_M: 0.03441, mse_TE: 0.01421\n",
      "Epoch:    1, mse_T: 1.72686, mse_E: 0.40777, mse_M: 0.03751, mse_TE: 0.01417\n",
      "Epoch:    2, mse_T: 1.72686, mse_E: 0.40181, mse_M: 0.03682, mse_TE: 0.01156\n",
      "Epoch:    3, mse_T: 1.72686, mse_E: 0.40334, mse_M: 0.03280, mse_TE: 0.01293\n",
      "Epoch:    4, mse_T: 1.72686, mse_E: 0.40487, mse_M: 0.04407, mse_TE: 0.01334\n",
      "\n",
      "Expect nothing to change:--------------\n",
      "Epoch:    0, mse_T: 1.72686, mse_E: 0.38620, mse_M: 0.01893, mse_TE: 0.00850\n",
      "Epoch:    1, mse_T: 1.72686, mse_E: 0.38620, mse_M: 0.01893, mse_TE: 0.00850\n",
      "Epoch:    2, mse_T: 1.72686, mse_E: 0.38620, mse_M: 0.01893, mse_TE: 0.00850\n",
      "Epoch:    3, mse_T: 1.72686, mse_E: 0.38620, mse_M: 0.01893, mse_TE: 0.00850\n",
      "Epoch:    4, mse_T: 1.72686, mse_E: 0.38620, mse_M: 0.01893, mse_TE: 0.00850\n",
      "\n",
      "Expect mse_TE to be the same, but mse_E and mse_M to change:--------------\n",
      "Epoch:    0, mse_T: 1.72686, mse_E: 0.38620, mse_M: 0.01893, mse_TE: 0.00850\n",
      "Epoch:    1, mse_T: 1.72686, mse_E: 0.38546, mse_M: 0.01778, mse_TE: 0.00850\n",
      "Epoch:    2, mse_T: 1.72686, mse_E: 0.38474, mse_M: 0.01655, mse_TE: 0.00850\n",
      "Epoch:    3, mse_T: 1.72686, mse_E: 0.38420, mse_M: 0.01573, mse_TE: 0.00850\n",
      "Epoch:    4, mse_T: 1.72686, mse_E: 0.38376, mse_M: 0.01545, mse_TE: 0.00850\n",
      "\n",
      "Expect mse_T to be the same.\n",
      "Condition checks equality of zT through Decoder_E on successive epochs:--------------\n",
      "Epoch:    0, mse_T: 1.72686, mse_E: 0.38331, mse_M: 0.01555, mse_TE: 0.00850\n",
      "Epoch:    1, mse_T: 1.72686, mse_E: 0.38344, mse_M: 0.01558, mse_TE: 0.00849\n",
      "decoder_E(zT) same as previous step is True\n",
      "Epoch:    2, mse_T: 1.72686, mse_E: 0.38366, mse_M: 0.01565, mse_TE: 0.00848\n",
      "decoder_E(zT) same as previous step is True\n",
      "Epoch:    3, mse_T: 1.72686, mse_E: 0.38397, mse_M: 0.01574, mse_TE: 0.00847\n",
      "decoder_E(zT) same as previous step is True\n",
      "Epoch:    4, mse_T: 1.72686, mse_E: 0.38434, mse_M: 0.01586, mse_TE: 0.00846\n",
      "decoder_E(zT) same as previous step is True\n"
     ]
    }
   ],
   "source": [
    "#Checks to make sure training works as expected:\n",
    "#train_T and train_E control the dropout and noise addition\n",
    "fine_tune_epochs = 20\n",
    "best_loss = np.inf\n",
    "test_iters = 5\n",
    "epoch = 1\n",
    "\n",
    "#Full network train step:\n",
    "print('Expect everything to change:--------------')\n",
    "for epoch in range(test_iters):\n",
    "    zT, zE, XrT, XrE = train_fn(XT=train_T_dat, XE=train_E_dat, train_T=True, train_E=True, subnetwork='all')\n",
    "    train_log_name, train_log_values = report_losses(train_T_dat, train_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='train_', verbose=True)\n",
    "\n",
    "#Switch off data augmentation for the T arm and update only E arm:\n",
    "print('\\nExpect mse_T to be unchanged:--------------')\n",
    "for epoch in range(test_iters):\n",
    "    zT, zE, XrT, XrE = train_fn(XT=train_T_dat, XE=train_E_dat, train_T=False, train_E=True, subnetwork='E')\n",
    "    train_log_name, train_log_values = report_losses(train_T_dat, train_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='train_', verbose=True)\n",
    "\n",
    "    \n",
    "#Switch off data augmentation for the T arm and update only E arm:\n",
    "print('\\nExpect nothing to change:--------------')\n",
    "for epoch in range(test_iters):\n",
    "    zT, zE, XrT, XrE = train_fn(XT=train_T_dat, XE=train_E_dat, train_T=False, train_E=False, subnetwork=None)\n",
    "    train_log_name, train_log_values = report_losses(train_T_dat, train_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='train_', verbose=True)\n",
    "            \n",
    "        \n",
    "#Testing only Encoder or decoder to change:\n",
    "print('\\nExpect mse_TE to be the same, but mse_E and mse_M to change:--------------')\n",
    "for epoch in range(test_iters):\n",
    "    zT, zE, XrT, XrE = finetune_fn(XT=train_T_dat, XE=train_E_dat, train_T=False, train_E=False, subnetwork='Decoder_E')\n",
    "    train_log_name, train_log_values = report_losses(train_T_dat, train_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='train_', verbose=True)\n",
    "\n",
    "print('\\nExpect mse_T to be the same.\\n'\n",
    "      'Condition checks equality of zT through Decoder_E on successive epochs:--------------')\n",
    "for epoch in range(test_iters):\n",
    "    #Running T representation through the E decoder:\n",
    "    zT, zE, XrT, XrE = finetune_fn(XT=train_T_dat, XE=train_E_dat, train_T=False, train_E=False, subnetwork='Encoder_E')\n",
    "    train_log_name, train_log_values = report_losses(train_T_dat, train_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='train_', verbose=True)\n",
    "    \n",
    "    if epoch==0:\n",
    "        zE = model_TE.decoder_E(zT, training=False)\n",
    "        zEref = zE.numpy().copy()\n",
    "    else:\n",
    "        zEcurrent = model_TE.decoder_E(zT, training=False).numpy()\n",
    "        print('decoder_E(zT) same as previous step is {}'.format(np.array_equal(zEref,zEcurrent)))\n",
    "        zEref = zEcurrent.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample loop to store best weights in the fine tuning step.\n",
    "\n",
    "fine_tune_epochs = 20\n",
    "best_loss = np.inf\n",
    "\n",
    "#Fine tune E decoder:\n",
    "for epoch in range(fine_tune_epochs):\n",
    "    #Training with train_E=True was found to prevent overfitting (as per validation losses)\n",
    "    zT, zE, XrT, XrE = finetune_fn(XT=train_T_dat, XE=train_E_dat, train_T=False, train_E=True, subnetwork='Decoder_E')\n",
    "    train_log_name, train_log_values = report_losses(train_T_dat, train_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='train_', verbose=False)\n",
    "            \n",
    "    #Collect validation metrics\n",
    "    zT, zE, XrT, XrE = finetune_fn(XT=val_T_dat, XE=val_E_dat, train_T=False, train_E=False, subnetwork=None)\n",
    "    val_log_name, val_log_values = report_losses(val_T_dat, val_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='val_', verbose=False)\n",
    "    \n",
    "    loss_dict = dict(zip(val_log_name, val_log_values))\n",
    "    if best_loss>loss_dict['val_mse_E']+loss_dict['val_mse_M']:\n",
    "        best_loss = loss_dict['val_mse_E']+loss_dict['val_mse_M']\n",
    "        val_log_name, val_log_values = report_losses(val_T_dat, val_E_dat, zT, zE, XrT, XrE, epoch=epoch, datatype='val_', verbose=True)\n",
    "        print('{:0.5f} is best val_mse_TE'.format(best_loss))\n",
    "        model_TE.save_weights(dir_pth['result']+fileid+'-best_E_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
