{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ae_model_def import Model_E_classifier\n",
    "from data_funcs import TE_get_splits_45\n",
    "import scipy.io as sio\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_paths(exp_name='TEMP'):\n",
    "    from pathlib import Path   \n",
    "    dir_pth = {}\n",
    "    curr_path = str(Path().absolute())\n",
    "    if '/Users/fruity' in curr_path:\n",
    "        base_path = '/Users/fruity/Dropbox/AllenInstitute/CellTypes/'\n",
    "        dir_pth['data'] = base_path + 'dat/raw/patchseq-v4/'\n",
    "    elif '/home/rohan' in curr_path:\n",
    "        base_path = '/home/rohan/Dropbox/AllenInstitute/CellTypes/'\n",
    "        dir_pth['data'] = base_path + 'dat/raw/patchseq-v4/'\n",
    "    elif '/allen' in curr_path:\n",
    "        base_path = '/allen/programs/celltypes/workgroups/mousecelltypes/Rohan/'\n",
    "        dir_pth['data'] = base_path + 'dat/raw/patchseq-v4/'\n",
    "\n",
    "    dir_pth['result'] =     base_path + 'dat/result/' + exp_name + '/'\n",
    "    dir_pth['checkpoint'] = dir_pth['result'] + 'checkpoints/'\n",
    "    dir_pth['logs'] =       dir_pth['result'] + 'logs/'\n",
    "\n",
    "    Path(dir_pth['logs']).mkdir(parents=True, exist_ok=True) \n",
    "    Path(dir_pth['checkpoint']).mkdir(parents=True, exist_ok=True) \n",
    "    return dir_pth\n",
    "\n",
    "\n",
    "class Datagen():\n",
    "    \"\"\"Iterator class to sample the dataset. Tensors T_dat and E_dat are provided at runtime.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxsteps, batchsize, E_dat, E_cat):\n",
    "        self.E_cat = E_cat\n",
    "        self.E_dat = E_dat\n",
    "        self.batchsize = batchsize\n",
    "        self.maxsteps = maxsteps\n",
    "        self.n_samples = self.E_dat.shape[0]\n",
    "        self.count = 0\n",
    "        print('Initializing generator...')\n",
    "        return\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count < self.maxsteps:\n",
    "            self.count = self.count+1\n",
    "            ind = np.random.randint(0, self.n_samples, self.batchsize)\n",
    "            return (tf.constant(self.E_dat[ind, :],dtype=tf.float32), tf.constant(self.E_cat[ind, :],dtype=tf.float32)) \n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batchsize=200\n",
    "cvfold=1\n",
    "Edat = 'pcifpx'\n",
    "latent_dim=3\n",
    "n_epochs=1000\n",
    "n_steps_per_epoch=500\n",
    "ckpt_save_freq=500\n",
    "run_iter=0\n",
    "model_id='E_class_v1_'\n",
    "exp_name = 'E_classifier'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_pth = set_paths(exp_name=exp_name)\n",
    "D = sio.loadmat(dir_pth['data']+'PS_v5_beta_0-4_pc_scaled_ipxf_eqTE.mat',squeeze_me=True)\n",
    "\n",
    "fileid = model_id + \\\n",
    "    '_bs_' + str(batchsize) + \\\n",
    "    '_se_' + str(n_steps_per_epoch) +\\\n",
    "    '_ne_' + str(n_epochs) + \\\n",
    "    '_cv_' + str(cvfold) + \\\n",
    "    '_ri_' + str(run_iter)\n",
    "fileid = fileid.replace('.', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fruity/miniconda3/envs/tf21-cpu/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=50.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "Edat = 'E_pcipxf'\n",
    "    \n",
    "#Data operations and definitions:\n",
    "D = sio.loadmat(dir_pth['data']+'PS_v5_beta_0-4_pc_scaled_ipxf_eqTE.mat',squeeze_me=True)\n",
    "D['E_pcipxf'] = np.concatenate([D['E_pc_scaled'],D['E_feature']],axis = 1)\n",
    "train_ind,val_ind,test_ind = TE_get_splits_45(matdict=D,cvfold=cvfold)\n",
    "\n",
    "Partitions = {'train_ind':train_ind,'val_ind':val_ind,'test_ind':test_ind}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "sorted_labels = sorted(list(set(zip(D['cluster'],D['cluster_id']))), key=lambda x: x[1])\n",
    "sorted_labels = [l[0] for l in sorted_labels]\n",
    "mlb = MultiLabelBinarizer(classes=sorted_labels)\n",
    "D['E_cat'] = mlb.fit_transform([{x} for x in D['cluster']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mlb.inverse_transform(D['E_cat'])\n",
    "pred = np.array([x[0] for x in pred])\n",
    "assert np.array_equal(pred,D['cluster']), 'Binarizer not working as expected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_E_dat = D[Edat][train_ind,:]\n",
    "train_E_cat = D['E_cat'][train_ind,:].astype(float)\n",
    "\n",
    "val_E_dat = D[Edat][val_ind,:]\n",
    "val_E_cat = D['E_cat'][val_ind,:].astype(float)\n",
    "\n",
    "    \n",
    "Edat_var = np.nanvar(D[Edat],axis=0)\n",
    "maxsteps = tf.constant(n_epochs*n_steps_per_epoch)\n",
    "batchsize = tf.constant(batchsize)\n",
    "    \n",
    "#Model definition\n",
    "optimizer_main = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "train_generator = tf.data.Dataset.from_generator(Datagen,output_types=(tf.float32, tf.float32),\n",
    "                                                 args=(maxsteps,batchsize,train_E_dat,train_E_cat))\n",
    "\n",
    "model = Model_E_classifier(E_output_dim=train_E_dat.shape[1],\n",
    "                           E_intermediate_dim=40,\n",
    "                           E_gauss_noise_wt=1.0,\n",
    "                           E_gnoise_sd=0.05,\n",
    "                           E_dropout=0.1,\n",
    "                           latent_dim=3,\n",
    "                           n_labels=train_E_cat.shape[1])\n",
    "\n",
    "#Model training functions \n",
    "@tf.function\n",
    "def train_fn(model, optimizer, XE, cE, train_E=False):\n",
    "    \"\"\"Enclose this with tf.function to create a fast training step. Function can be used for inference as well. \n",
    "    Arguments:\n",
    "        cE: E category for training or validation\n",
    "        XE: E data for training or validation\n",
    "        train_T: {bool} -- Switch augmentation for T data on or off\n",
    "        train_E {bool} -- Switch augmentation for E data on or off\n",
    "        subnetwork {str} -- 'all' or 'E'. 'all' trains the full network, 'E' trains only the E arm.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        zE,_ = model((XE, cE),train_E=train_E)\n",
    "        trainable_weights = [weight for weight in model.trainable_weights]\n",
    "        loss = sum(model.losses)\n",
    "\n",
    "    grads = tape.gradient(loss, trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "    return zE\n",
    "\n",
    "#Model logging functions\n",
    "def report_losses(model, epoch, datatype='train', acc = 0, verbose=False):\n",
    "    ce_loss = model.ce_loss.numpy()\n",
    "    if verbose:\n",
    "        print(f'Epoch:{epoch:5d}, ce_loss: {ce_loss:0.3f}, acc: {acc:0.3f}')\n",
    "\n",
    "    log_name = [datatype+i for i in ['epoch','ce_loss','acc']]\n",
    "    log_values = [epoch, ce_loss, acc]\n",
    "    return log_name, log_values\n",
    "\n",
    "def save_results(this_model,Data,fname,Inds=Partitions,Edat=Edat,mlb=mlb):\n",
    "    all_E_dat = tf.constant(D[Edat],dtype=tf.float32)\n",
    "    all_E_cat = tf.constant(D['E_cat'],dtype=tf.float32)\n",
    "    zE, oH = model((all_E_dat, all_E_cat), training=False)\n",
    "    oH = oH.numpy()\n",
    "    bin_oH = (oH == np.max(oH,axis=1,keepdims=True)).astype(int)\n",
    "    pred_E_cat = mlb.inverse_transform(bin_oH)\n",
    "    pred_E_cat = np.array([x[0] for x in pred_E_cat])\n",
    "    acc = (np.sum(pred_E_cat==D['cluster'])/D['cluster'].size)*100\n",
    "    savemat = {'zE':zE.numpy(), 'pred_E_cat':pred_E_cat}\n",
    "    savemat.update(Inds)\n",
    "    sio.savemat(fname, savemat, do_compression=True)\n",
    "    return\n",
    "\n",
    "#Main training loop ----------------------------------------------------------------------\n",
    "epoch=0\n",
    "best_val_acc = 0\n",
    "\n",
    "for step, (XE, cE) in enumerate(train_generator): \n",
    "    train_fn(model=model, optimizer=optimizer_main, XE=XE, cE=cE, train_E=True)\n",
    "\n",
    "    if (step+1) % n_steps_per_epoch == 0:\n",
    "        #Update epoch count\n",
    "        epoch = epoch+1\n",
    "\n",
    "        #Collect training metrics\n",
    "        _,train_pred = model((train_E_dat, train_E_cat), train_E=False)\n",
    "        train_pred = train_pred.numpy()\n",
    "        train_pred = (train_pred == np.max(train_pred,axis=1,keepdims=True)).astype(int)\n",
    "        train_acc = (np.sum(np.multiply(train_pred,train_E_cat))/train_pred.shape[0])\n",
    "        train_log_name, train_log_values = report_losses(model=model ,epoch=epoch,  acc=train_acc, datatype='train_', verbose=True)\n",
    "\n",
    "        #Collect validation metrics\n",
    "        _, val_pred = model((val_E_dat, val_E_cat), train_E=False)\n",
    "\n",
    "        val_pred = val_pred.numpy()\n",
    "        val_pred = (val_pred == np.max(val_pred,axis=1,keepdims=True)).astype(int)\n",
    "        val_acc = (np.sum(np.multiply(val_pred,val_E_cat))/val_pred.shape[0])\n",
    "        val_log_name, val_log_values = report_losses(model=model, epoch=epoch, acc=val_acc, datatype='val_', verbose=True)\n",
    "\n",
    "        with open(dir_pth['logs']+fileid+'.csv', \"a\") as logfile:\n",
    "            writer = csv.writer(logfile, delimiter=',')\n",
    "            #Write headers to the log file\n",
    "            if epoch == 1:\n",
    "                writer.writerow(train_log_name+val_log_name)\n",
    "            writer.writerow(train_log_values+val_log_values)\n",
    "\n",
    "        if epoch % ckpt_save_freq == 0:\n",
    "            #Save model weights\n",
    "            model.save_weights(dir_pth['checkpoint']+fileid+'_ckptep_'+str(epoch)+'-weights.h5')\n",
    "            #Save reconstructions and results for the full dataset:\n",
    "            save_results(this_model=model,Data=D,fname=dir_pth['checkpoint']+fileid+'_ckptep_'+str(epoch)+'-summary.mat')\n",
    "            \n",
    "        if (val>best_val_acc) and (epoch>20):\n",
    "            #Save best accuracy model weights\n",
    "            model.save_weights(dir_pth['result']+fileid+'-weights.h5')\n",
    "\n",
    "            #Save reconstructions and results for the full dataset:\n",
    "            save_results(this_model=model,Data=D,fname=dir_pth['result']+fileid+'-summary.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
